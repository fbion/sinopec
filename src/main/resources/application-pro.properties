
server.address=0.0.0.0
server.port=88


#JWT 认证配置
jwt.header=Authorization
jwt.secret=Sinopec@&123!123?
#token不过期时间默认5分钟 设置成7天
jwt.expiration=604800
jwt.tokenHead=Bearer
#druid需要忽略的url(只有druidEnabled=true才起作用)
jwt.druidIgnoreUrls=/druid/**,/favicon.ico
#swagger需要忽略的url(只有swaggerEnabled=true才起作用)
jwt.swaggerIgnoreUrls=/swagger-ui.html,/swagger-resources/**,/webjars/**,/v2/api-docs,/configuration/ui,/configuration/security
#common需要忽略的url(只有commonEnabled=true才起作用)
jwt.commonIgnoreUrls=/service/auth/login
#基础授权的url(basicEnabled)
jwt.basicAuthenticatedUrls=/service/mobileCollectTask/**,/service/auth/logout
#是否开启swagger
jwt.swaggerEnabled=true
#是否开启druid
jwt.druidEnabled=true
#是否忽略特定请求
jwt.commonEnabled=true
#是否授权特定请求
jwt.basicEnabled=true

#postgresql    com.alibaba.druid.pool.DruidDataSource（不支持分布式事物）
spring.datasource.type=com.alibaba.druid.pool.xa.DruidXADataSource

spring.datasource.druid.console.name=admin
spring.datasource.druid.console.password=123
spring.datasource.druid.console.log=123
spring.datasource.druid.console.logSlowSql=true

spring.datasource.one.druid.name=druid1
spring.datasource.one.druid.driverClassName=org.postgresql.Driver
spring.datasource.one.druid.url=jdbc:postgresql://192.168.12.14:5432/deepdata_bi?useSSL=false
spring.datasource.one.druid.username=postgres
spring.datasource.one.druid.password=123456

# 连接池配置,下面配置说明请参考Druid Github Wiki，配置_DruidDataSource参考配置
spring.datasource.one.druid.initialSize=1
spring.datasource.one.druid.maxActive=20
spring.datasource.one.druid.minIdle=1
spring.datasource.one.druid.maxWait=60000
spring.datasource.one.druid.validationQuery=SELECT 1
spring.datasource.one.druid.testOnBorrow=false
spring.datasource.one.druid.testOnReturn=false
spring.datasource.one.druid.testWhileIdle=true
spring.datasource.one.druid.timeBetweenEvictionRunsMillis=60000
spring.datasource.one.druid.minEvictableIdleTimeMillis=30000

# 打开PSCache，并且指定每个连接上PSCache的大小
spring.datasource.one.druid.poolPreparedStatements=true
spring.datasource.one.druid.maxPoolPreparedStatementPerConnectionSize=20
# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
spring.datasource.one.druid.filters=stat,wall,log4j2
# 通过connectProperties属性来打开mergeSql功能；慢SQL记录
spring.datasource.one.druid.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
spring.datasource.one.druid.logSlowSql=true
# 合并多个DruidDataSource的监控数据
spring.datasource.one.druid.useGlobalDataSourceStat=true

# 配置log4j2日志输出(Druid内置提供了四种LogFilter：Log4jFilter、Log4j2Filter、CommonsLogFilter、Slf4jLogFilter)
spring.datasource.one.druid.filter.log4j2.enabled=true
spring.datasource.one.druid.filter.log4j2.statement-create-after-log-enabled=false
spring.datasource.one.druid.filter.log4j2.statement-close-after-log-enabled=false
spring.datasource.one.druid.filter.log4j2.result-set-open-after-log-enabled=false
spring.datasource.one.druid.filter.log4j2.result-set-close-after-log-enabled=false


#第二个数据源
spring.datasource.two.druid.name=druid2
spring.datasource.two.druid.driverClassName=org.postgresql.Driver
spring.datasource.two.druid.url=jdbc:postgresql://192.168.12.14:5432/deepdata_dt?useSSL=false
spring.datasource.two.druid.username=postgres
spring.datasource.two.druid.password=123456

# 连接池配置,下面配置说明请参考Druid Github Wiki，配置_DruidDataSource参考配置
spring.datasource.two.druid.initialSize=1
spring.datasource.two.druid.maxActive=20
spring.datasource.two.druid.minIdle=1
spring.datasource.two.druid.maxWait=60000
spring.datasource.two.druid.validationQuery=SELECT 1
spring.datasource.two.druid.testOnBorrow=false
spring.datasource.two.druid.testOnReturn=false
spring.datasource.two.druid.testWhileIdle=true
spring.datasource.two.druid.timeBetweenEvictionRunsMillis=60000
spring.datasource.two.druid.minEvictableIdleTimeMillis=30000

# 打开PSCache，并且指定每个连接上PSCache的大小
spring.datasource.two.druid.poolPreparedStatements=true
spring.datasource.two.druid.maxPoolPreparedStatementPerConnectionSize=20
# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
spring.datasource.two.druid.filters=stat,wall,log4j2
# 通过connectProperties属性来打开mergeSql功能；慢SQL记录
spring.datasource.two.druid.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
spring.datasource.two.druid.logSlowSql=true
# 合并多个DruidDataSource的监控数据
spring.datasource.two.druid.useGlobalDataSourceStat=true

# 配置log4j2日志输出(Druid内置提供了四种LogFilter：Log4jFilter、Log4j2Filter、CommonsLogFilter、Slf4jLogFilter)
spring.datasource.two.druid.filter.log4j2.enabled=true
spring.datasource.two.druid.filter.log4j2.statement-create-after-log-enabled=false
spring.datasource.two.druid.filter.log4j2.statement-close-after-log-enabled=false
spring.datasource.two.druid.filter.log4j2.result-set-open-after-log-enabled=false
spring.datasource.two.druid.filter.log4j2.result-set-close-after-log-enabled=false



#============== kafka ===================
# 指定kafka 主题devo-topic1
kafka.faceTopic=devo-topic5
kafka.vehicleTopic=index-vehicle5
#=============== kafka producer  ==================
# 生产者kafka地址，多个以“,”隔开
spring.kafka.producer.bootstrap-servers=192.168.12.14:9092,192.168.12.14:9093,192.168.12.14:9094
spring.kafka.producer.retries=1
# 每次批量发送消息的数量
spring.kafka.producer.batch-size=16384
spring.kafka.producer.buffer-memory=33554432
# 指定消息key和消息体的编解码方式
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=com.sensenets.sinopec.buiness.kafka.ProtoObjSerializer

#===============kafka consumer  ====================
# 指定消息key和消息体的编解码方式
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=com.sensenets.sinopec.buiness.kafka.ProtoObjDeserializer

# 消费者kafka地址，多个以“,”隔开
spring.kafka.consumer.bootstrap-servers=192.168.12.14:9092,192.168.12.14:9093,192.168.12.14:9094
# 指定默认消费者group id
spring.kafka.consumer.group-id=all-object-group
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval=100
# 批量获取每次最多100条
spring.kafka.consumer.max-poll-records=100
# 消费者监听器线程数
spring.kafka.listener.concurrency=3
# 开启消费者监听器批量获取
spring.kafka.listener.type=batch
# 拉取记录超时时间
spring.kafka.listener.poll-timeout=3000



