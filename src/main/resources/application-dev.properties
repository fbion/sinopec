
server.address=0.0.0.0
server.port=88


#JWT 认证配置
jwt.header=Authorization
jwt.secret=Sinopec123_123
#token七天不过期
jwt.expiration=604800
jwt.tokenHead=Bearer
jwt.exceptUrl=/service/auth/login


#postgresql
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
spring.datasource.druid.name=druid
spring.datasource.druid.driverClassName=org.postgresql.Driver
spring.datasource.druid.url=jdbc:postgresql://localhost:5432/thor?useSSL=false
spring.datasource.druid.username=postgres
spring.datasource.druid.password=123456

# 连接池配置,下面配置说明请参考Druid Github Wiki，配置_DruidDataSource参考配置
spring.datasource.druid.initialSize=1
spring.datasource.druid.maxActive=20
spring.datasource.druid.minIdle=1
spring.datasource.druid.maxWait=60000
spring.datasource.druid.validationQuery=SELECT 1
spring.datasource.druid.testOnBorrow=false
spring.datasource.druid.testOnReturn=false
spring.datasource.druid.testWhileIdle=true
spring.datasource.druid.timeBetweenEvictionRunsMillis=60000
spring.datasource.druid.minEvictableIdleTimeMillis=30000

# 打开PSCache，并且指定每个连接上PSCache的大小
spring.datasource.druid.poolPreparedStatements=true
spring.datasource.druid.maxPoolPreparedStatementPerConnectionSize=20
# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
spring.datasource.druid.filters=stat,wall,log4j2
# 通过connectProperties属性来打开mergeSql功能；慢SQL记录
spring.datasource.druid.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
spring.datasource.druid.logSlowSql=true

# 配置log4j2日志输出(Druid内置提供了四种LogFilter：Log4jFilter、Log4j2Filter、CommonsLogFilter、Slf4jLogFilter)
spring.datasource.druid.filter.log4j2.enabled=true
spring.datasource.druid.filter.log4j2.statement-create-after-log-enabled=false
spring.datasource.druid.filter.log4j2.statement-close-after-log-enabled=false
spring.datasource.druid.filter.log4j2.result-set-open-after-log-enabled=false
spring.datasource.druid.filter.log4j2.result-set-close-after-log-enabled=false






#============== kafka ===================
# 指定kafka 主题
kafka.faceTopic=devo-topic
kafka.vehicleTopic=index-vehicle2
#=============== kafka producer  ==================
# 生产者kafka地址，多个以“,”隔开
spring.kafka.producer.bootstrap-servers=192.168.12.13:9092,192.168.12.13:9093,192.168.12.13:9094
spring.kafka.producer.retries=1
# 每次批量发送消息的数量
spring.kafka.producer.batch-size=16384
spring.kafka.producer.buffer-memory=33554432
# 指定消息key和消息体的编解码方式
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#===============kafka consumer  ====================
# 指定消息key和消息体的编解码方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# 消费者kafka地址，多个以“,”隔开
spring.kafka.consumer.bootstrap-servers=192.168.12.13:9092,192.168.12.13:9093,192.168.12.13:9094
# 指定默认消费者group id
spring.kafka.consumer.group-id=all-object-group
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval=100
# 批量获取每次最多100条
spring.kafka.consumer.max-poll-records=100
# 消费者监听器线程数
spring.kafka.listener.concurrency=3
# 开启消费者监听器批量获取
spring.kafka.listener.type=batch
# 拉取记录超时时间
spring.kafka.listener.poll-timeout=3000



